{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import (Dense, Conv1D)\n",
    "from keras.models import Sequential\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "id": "_OENIgutwjp1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read the natural language understanding dataset and BERT model\n",
    "\n",
    "Clone the repos inside intent-detection directory\n",
    "```\n",
    "git clone https://github.com/tilde-nlp/NLU-datasets.git\n",
    "git clone https://huggingface.co/bert-base-multilingual-cased\n",
    "```\n",
    "\n",
    "Directory tree should be as follows\n",
    "```\n",
    "/intent-detection\n",
    "├── NLU-datasets\n",
    "├── bert-base-multilingual-cased\n",
    "├── run-on-windows.ipynb\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if \"NLU-datasets\" not in os.getcwd():\n",
    "    os.chdir(\"./NLU-datasets\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_data(path: str) -> List[str]:\n",
    "    \"\"\" Read path and append each line without \\n as an element to an array.\n",
    "    Encoding is specified to correctly read files in Russian.\n",
    "    Example output: ['FindConnection', 'FindConnection', ..., 'FindConnection']\n",
    "    \"\"\"\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        array = []\n",
    "        for line in list(f):\n",
    "            array.append(line.split('\\n')[0])\n",
    "        return array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_list = Path(\"chatbot\").glob(\"**/*.txt\")\n",
    "\n",
    "for path in path_list:\n",
    "    # because path is object not string\n",
    "    path_in_str = str(path)\n",
    "    # print(path_in_str)\n",
    "    if path_in_str == \"chatbot\\chatbot_train_ans.txt\":\n",
    "        train_answers = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\chatbot_test_ans.txt\":\n",
    "        test_answers  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\en\\chatbot_test_q.txt\":\n",
    "        en_test  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\en\\chatbot_train_q.txt\":\n",
    "        en_train  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\lv\\chatbot_test_q.txt\":\n",
    "        lv_test  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\lv\\chatbot_train_q.txt\":\n",
    "        lv_train  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\\\ru\\chatbot_test_q.txt\":\n",
    "        ru_test  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\\\ru\\chatbot_train_q.txt\":\n",
    "        ru_train  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\et\\chatbot_test_q.txt\":\n",
    "        et_test  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\et\\chatbot_train_q.txt\":\n",
    "        et_train  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\lt\\chatbot_test_q.txt\":\n",
    "        lt_test  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\lt\\chatbot_train_q.txt\":\n",
    "        lt_train  = get_data(path_in_str)\n",
    "\n",
    "\n",
    "print(train_answers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert len(train_answers) == len(en_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if \"NLU-datasets\" in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# define model and tokenizer\n",
    "model_name = \"bert-base-multilingual-cased\" # loading from huggingface\n",
    "model_name = \"./bert-base-multilingual-cased\" # loading from local path\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model_bert = TFBertModel.from_pretrained(model_name)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDWtw2FHxDOl",
    "outputId": "d583acfa-dc0e-465b-bd41-e5f2bc4477e3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def create_model_one_layer(sentence_length: int, units: int = 2, hidden_size: int = 768):\n",
    "    \"\"\"\n",
    "    returns <tf.Tensor: shape=(1, 1, units), dtype=float32>\n",
    "    e.g. <tf.Tensor: shape=(1, 1, 2), dtype=float32>\n",
    "    where 2 = units\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=(sentence_length, hidden_size)))\n",
    "    model.add(Dense(units, activation='softmax'))\n",
    "    model.add(Conv1D(units, sentence_length, padding=\"valid\", activation=\"softmax\"))\n",
    "    model.add(Dense(units, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_adam_optimizer(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0, epsilon=None, amsgrad=False):\n",
    "    # TODO: Replace legacy optimizer with current version of Adam\n",
    "    return tf.keras.optimizers.legacy.Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, decay=decay, amsgrad=amsgrad)\n",
    "\n",
    "\n",
    "# use keras.to_categorical() instead\n",
    "def encode_labels(answers: List) -> List:\n",
    "    \"\"\" Encode labels in one hot-encoding\n",
    "    'FindConnection' corresponds to [[1, 0]]\n",
    "    'DepartureTime' corresponds to [[0, 1]]\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    for answer in answers:\n",
    "        if answer == 'FindConnection':\n",
    "            y.append([[1, 0]])\n",
    "        else:\n",
    "            y.append([[0, 1]])\n",
    "    return y"
   ],
   "metadata": {
    "id": "wB3HbmHb2Ugq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = encode_labels(train_answers)\n",
    "labels_expanded = tf.convert_to_tensor(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test on a small example\n",
    "\n",
    "## Sentence -> word embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "sentence_length = 20\n",
    "\n",
    "text = en_train[0:batch_size]\n",
    "encoded_input = tokenizer(text, padding='max_length', max_length=sentence_length, truncation=True, return_tensors='tf')\n",
    "encoded_input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# odict_keys(['last_hidden_state', 'pooler_output'])\n",
    "inputs = model_bert(encoded_input)[\"last_hidden_state\"]\n",
    "inputs.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word embedding -> class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rate = 0.03\n",
    "optimizer = create_adam_optimizer(lr=learning_rate)\n",
    "classification_model = create_model_one_layer(sentence_length=sentence_length)\n",
    "\n",
    "classification_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# initial probabilities\n",
    "classification_model(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "labels_expanded = tf.convert_to_tensor(labels[0:batch_size])\n",
    "\n",
    "classification_model.fit(inputs, y=labels_expanded, epochs=epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# view the output of the classification_model: probabilities for labels\n",
    "\n",
    "classification_model(inputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run on training dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = encode_labels(train_answers)\n",
    "labels_expanded = tf.convert_to_tensor(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_classification_model(learning_rate: int, sentence_length: int):\n",
    "    optimizer = create_adam_optimizer(lr=learning_rate)\n",
    "    classification_model = create_model_one_layer(sentence_length=sentence_length)\n",
    "\n",
    "    classification_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return classification_model\n",
    "\n",
    "\n",
    "def plot_performance(data, dataset: str, x_label: str = 'accuracy'):\n",
    "    plt.plot(data)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel('epochs')\n",
    "    ax.set_ylabel(x_label)\n",
    "    plt.title(f\"model {x_label}\")\n",
    "    plt.savefig(f\"{dataset}-{x_label}.png\")\n",
    "    # plt.savefig(f\"{dataset}{x_label}.pdf\", dpi=150) # pdf for LaTeX\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def training(train_dataset, dataset_name: str, learning_rate: int, sentence_length: int, labels_expanded=labels_expanded):\n",
    "\n",
    "    classification_model = get_classification_model(learning_rate, sentence_length)\n",
    "\n",
    "    encoded_input = tokenizer(train_dataset, padding='max_length', max_length=sentence_length, truncation=True, return_tensors='tf')\n",
    "    classification_input = model_bert(encoded_input)[\"last_hidden_state\"]\n",
    "\n",
    "    print(labels_expanded)\n",
    "    history = classification_model.fit(classification_input, y=labels_expanded, batch_size=batch_size, epochs=number_of_epochs)\n",
    "    # predictions = classification_model(classification_input)\n",
    "\n",
    "    plot_performance(history.history['accuracy'], dataset=dataset_name, x_label='accuracy')\n",
    "    plot_performance(history.history['loss'], dataset=dataset_name, x_label='loss')\n",
    "\n",
    "    return classification_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "sentence_length = 20\n",
    "learning_rate = 0.0003\n",
    "number_of_epochs = 150"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## English"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classification_model_en = training(en_train, dataset_name=\"en_train\", learning_rate=learning_rate, sentence_length=sentence_length)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
