{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import (Dense, Conv1D)\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import BertTokenizer, TFBertModel"
   ],
   "metadata": {
    "id": "_OENIgutwjp1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read the natural language understanding dataset and BERT model\n",
    "\n",
    "Clone the repos inside intent-detection directory\n",
    "```\n",
    "git clone https://github.com/tilde-nlp/NLU-datasets.git\n",
    "git clone https://huggingface.co/bert-base-multilingual-cased\n",
    "```\n",
    "\n",
    "Directory tree should be as follows\n",
    "```\n",
    "/intent-detection\n",
    "├── NLU-datasets\n",
    "├── bert-base-multilingual-cased\n",
    "├── run-on-windows.ipynb\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if \"NLU-datasets\" not in os.getcwd():\n",
    "    os.chdir(\"./NLU-datasets\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_data(path: str) -> List[str]:\n",
    "    \"\"\" Read path and append each line without \\n as an element to an array.\n",
    "    Encoding is specified to correctly read files in Russian.\n",
    "    Example output: ['FindConnection', 'FindConnection', ..., 'FindConnection']\n",
    "    \"\"\"\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        array = []\n",
    "        for line in list(f):\n",
    "            array.append(line.split('\\n')[0])\n",
    "        return array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_list = Path(\"chatbot\").glob(\"**/*.txt\")\n",
    "\n",
    "for path in path_list:\n",
    "    # because path is object not string\n",
    "    path_in_str = str(path)\n",
    "    # print(path_in_str)\n",
    "    if path_in_str == \"chatbot\\chatbot_train_ans.txt\":\n",
    "        train_answers = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\chatbot_test_ans.txt\":\n",
    "        test_answers  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\en\\chatbot_test_q.txt\":\n",
    "        en_test  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\en\\chatbot_train_q.txt\":\n",
    "        en_train  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\lv\\chatbot_test_q.txt\":\n",
    "        lv_test  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\lv\\chatbot_train_q.txt\":\n",
    "        lv_train  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\\\ru\\chatbot_test_q.txt\":\n",
    "        ru_test  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\\\ru\\chatbot_train_q.txt\":\n",
    "        ru_train  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\et\\chatbot_test_q.txt\":\n",
    "        et_test  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\et\\chatbot_train_q.txt\":\n",
    "        et_train  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\lt\\chatbot_test_q.txt\":\n",
    "        lt_test  = get_data(path_in_str)\n",
    "    elif path_in_str == \"chatbot\\lt\\chatbot_train_q.txt\":\n",
    "        lt_train  = get_data(path_in_str)\n",
    "\n",
    "\n",
    "print(train_answers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if \"NLU-datasets\" in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# define model and tokenizer\n",
    "model_name = \"bert-base-multilingual-cased\" # loading from huggingface\n",
    "model_name = \"./bert-base-multilingual-cased\" # loading from local path\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model_bert = TFBertModel.from_pretrained(model_name)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDWtw2FHxDOl",
    "outputId": "d583acfa-dc0e-465b-bd41-e5f2bc4477e3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing word embeddings on a small example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# test the tokenizer\n",
    "multiple_lines = [\n",
    "'i want to go marienplatz',\n",
    "'when is the next train in muncher freiheit?',\n",
    "'when does the next u-bahn leaves from garching forschungszentrum?'\n",
    "]\n",
    "ids_for_test = tokenizer(multiple_lines, padding=True, return_tensors='tf')\n",
    "ids_for_test"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogq1bdseEHhl",
    "outputId": "da6b6b8c-e752-4362-8c77-cfd08e49d2df"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# test the model\n",
    "model_bert_output = model_bert(ids_for_test)"
   ],
   "metadata": {
    "id": "w4RzE1OpIMDk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(model_bert_output.keys())\n",
    "\n",
    "input_dimensions = model_bert_output['last_hidden_state'].shape\n",
    "input_dimensions"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uirRKwL5dJoW",
    "outputId": "2c62d2da-dd57-4729-c662-f8fa9cd507d2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "assert len(train_answers) == len(en_train)"
   ],
   "metadata": {
    "id": "YAdAGm3lyq1u"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_model_one_layer(units: int, batch_size: int, sentence_length: int, **kwargs):\n",
    "    \"\"\"\n",
    "    returns <tf.Tensor: shape=(1, batch_size, 1, units), dtype=float32\n",
    "    e.g. <tf.Tensor: shape=(1, 4, 1, 2), dtype=float32\n",
    "    where 4 = batch_size, 2 = units\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=(batch_size, sentence_length, 768))) # from shape=(1, 9, 768)\n",
    "    model.add(Dense(units, activation='softmax'))\n",
    "    model.add(Conv1D(units, sentence_length, padding=\"valid\", activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_adam_optimizer(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0, epsilon=None, amsgrad=False):\n",
    "    return tf.keras.optimizers.legacy.Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, decay=decay, amsgrad=amsgrad)\n",
    "\n",
    "\n",
    "def encode_labels(answers: List) -> List:\n",
    "    \"\"\" Encode labels in one hot-encoding\n",
    "    'FindConnection' corresponds to [[1, 0]]\n",
    "    'DepartureTime' corresponds to [[0, 1]]\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    for answer in answers:\n",
    "        if answer == 'FindConnection':\n",
    "            y.append([[1, 0]])\n",
    "        else:\n",
    "            y.append([[0, 1]])\n",
    "    return y\n",
    "\n",
    "\n",
    "def expand_dimensions(y: List):\n",
    "    y_tensor = tf.convert_to_tensor(y, dtype=float)\n",
    "    return tf.expand_dims(y_tensor, axis=0)"
   ],
   "metadata": {
    "id": "wB3HbmHb2Ugq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test on small example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "sentence_length = 20\n",
    "\n",
    "text = en_train[0:batch_size]\n",
    "encoded_input = tokenizer(text, padding='max_length', max_length=sentence_length, truncation=True, return_tensors='tf')\n",
    "inputs = model_bert(encoded_input)[\"last_hidden_state\"]\n",
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rate = 0.03\n",
    "optimizer = create_adam_optimizer(lr=learning_rate)\n",
    " # units = 2 because we want to get scores for two classes\n",
    "classification_model = create_model_one_layer(units=2, batch_size=batch_size, sentence_length=sentence_length)\n",
    "\n",
    "classification_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    # loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# expand dimensions, why????\n",
    "classification_input = tf.expand_dims(inputs, axis=0)\n",
    "\n",
    "# view the output of the classification_model\n",
    "# probabilities for labels\n",
    "classification_model(classification_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels= encode_labels(train_answers)\n",
    "labels_expanded = expand_dimensions(labels[0:batch_size])\n",
    "\n",
    "labels_expanded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "number_of_epochs = 5\n",
    "classification_model.fit(classification_input, y=labels_expanded, epochs=number_of_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classification_model(classification_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run on all inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "sentence_length = 20\n",
    "learning_rate = 0.03\n",
    "number_of_epochs = 5\n",
    "\n",
    "optimizer = create_adam_optimizer(lr=learning_rate)\n",
    "classification_model = create_model_one_layer(units=2, batch_size=batch_size, sentence_length=sentence_length)\n",
    "\n",
    "classification_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    # loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for index in range(len(train_answers)//batch_size):\n",
    "    text = en_train[index*batch_size:(index+1)*batch_size]\n",
    "    encoded_input = tokenizer(text, padding='max_length', max_length=sentence_length, truncation=True, return_tensors='tf')\n",
    "    classification_input = tf.expand_dims(model_bert(encoded_input)[\"last_hidden_state\"], axis=0)\n",
    "\n",
    "    labels = encode_labels(train_answers)\n",
    "    labels_expanded = expand_dimensions(labels[index*batch_size:(index+1)*batch_size])\n",
    "    classification_model.fit(classification_input, y=labels_expanded, epochs=number_of_epochs)\n",
    "    # print(classification_model(classification_input))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
